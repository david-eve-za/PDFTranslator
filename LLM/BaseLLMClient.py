import os
from abc import ABC, abstractmethod
from typing import Any, Dict, Optional  # Import necessary types


# (Optional) You might want a base class for common configurations or utilities
# similar to how GeminiAI uses APIClient and CommonPrompts.
# class BaseLLMConfig:
#     def __init__(self, api_key: Optional[str] = None, model_name: Optional[str] = None):
#         self.api_key = api_key or os.getenv("YOUR_DEFAULT_API_KEY_ENV_VAR") # Example
#         self.model_name = model_name or "default-model"
#         if not self.api_key:
#             raise ValueError("API key must be provided either directly or via environment variable.")
#         # ... other common initializations ...

class BaseLLMClient(ABC):
    """
    Abstract Base Class defining the interface for Large Language Model clients.

    This class serves as a template, ensuring that any concrete LLM client
    implementation provides a consistent set of core functionalities. Subclasses
    must implement all methods decorated with @abstractmethod.
    """

    def __init__(self, model_name: str, api_key: Optional[str] = None, **kwargs):
        """
        Initializes the base LLM client.

        Subclasses should call super().__init__() and handle their specific
        setup, such as initializing the provider's client library, setting
        API keys, and configuring default generation parameters.

        Args:
            model_name (str): The identifier for the specific LLM model to be used.
            api_key (Optional[str]): The API key for the LLM provider. If None,
                                     the implementation should attempt to load it
                                     from environment variables or other config.
            **kwargs: Additional keyword arguments for provider-specific setup.
        """
        self._model_name = model_name
        self._api_key = api_key
        # Subclasses will likely initialize their specific client object here
        # e.g., self._provider_client = initialize_provider_sdk(api_key=self._api_key, ...)
        print(f"BaseLLMClient initialized for model: {self._model_name}")

    @property
    def model_name(self) -> str:
        """Returns the name of the model being used."""
        return self._model_name

    @abstractmethod
    def call_model(self, prompt: str, **kwargs) -> str:
        """
        Sends a prompt to the LLM and returns the generated text response.

        This is the core method for interacting with the LLM. Subclasses must
        implement the logic to communicate with their specific LLM provider's API.
        Rate limiting and error handling should ideally be implemented within this method
        or called from it.

        Args:
            prompt (str): The input text prompt for the LLM.
            **kwargs: Provider-specific parameters (e.g., temperature, max_tokens,
                      stop_sequences, system_prompt).

        Returns:
            str: The text response generated by the LLM.

        Raises:
            NotImplementedError: If the subclass does not implement this method.
            # Subclasses should raise more specific exceptions for API errors, etc.
        """
        raise NotImplementedError

    @abstractmethod
    def count_tokens(self, text: str) -> int:
        """
        Counts the number of tokens in a given text string according to the
        model's specific tokenizer.

        This is essential for managing context window limits, estimating costs,
        and implementing token-based rate limiting.

        Args:
            text (str): The text to tokenize and count.

        Returns:
            int: The total number of tokens in the text.

        Raises:
            NotImplementedError: If the subclass does not implement this method.
        """
        raise NotImplementedError

    # --- Optional but Recommended Methods ---

    def get_system_prompt_size(self) -> int:
        """
        Returns the size (in tokens) of the default system prompt, if any.

        This can be useful for calculating available space for user prompts
        within the context window. If no system prompt is used by default,
        this method can return 0.

        Returns:
            int: The token count of the default system prompt, or 0.
        """
        # Default implementation assumes no system prompt or size tracking here.
        # Subclasses using system prompts (like GeminiAI) should override this.
        return 0

    def get_generation_config(self) -> Dict[str, Any]:
        """
        Returns the default configuration used for generating responses.

        This might include parameters like temperature, top_p, max_output_tokens, etc.

        Returns:
            Dict[str, Any]: A dictionary of default generation parameters.
        """
        # Default implementation returns an empty dict.
        # Subclasses should override this to return their actual default config.
        return {}

    # --- Example of how Rate Limiting might be integrated (Optional) ---
    # You might have a separate ABC or Mixin for APIClient behavior

    # @abstractmethod
    # def wait_if_needed(self, requested_tokens: int = 0) -> None:
    #     """
    #     Checks rate limits and pauses execution if necessary before making an API call.
    #
    #     Args:
    #         requested_tokens (int): The estimated number of tokens for the upcoming request,
    #                                 if applicable to token-based limits.
    #     """
    #     raise NotImplementedError


# --- Example Usage (How to implement a concrete class) ---

class MyDummyLLMClient(BaseLLMClient):
    """
    A concrete implementation of BaseLLMClient for demonstration purposes.
    """

    def __init__(self, model_name: str = "dummy-model-v1", api_key: Optional[str] = "dummy_key", **kwargs):
        super().__init__(model_name=model_name, api_key=api_key, **kwargs)
        # Initialize dummy client specifics
        self._default_temp = kwargs.get("temperature", 0.5)
        print(f"MyDummyLLMClient initialized with temp: {self._default_temp}")
        # In a real scenario, initialize the provider's SDK client here
        # self._client = SomeProviderSDK.Client(api_key=self._api_key)

    def call_model(self, prompt: str, **kwargs) -> str:
        """Dummy implementation of calling the model."""
        print(f"Calling dummy model '{self.model_name}' with prompt: '{prompt[:50]}...'")
        # Simulate API call parameters
        temperature = kwargs.get("temperature", self._default_temp)
        max_tokens = kwargs.get("max_tokens", 100)
        print(f"  -> Using temp: {temperature}, max_tokens: {max_tokens}")

        # Simulate rate limiting (if needed)
        # self.wait_if_needed(self.count_tokens(prompt)) # Assuming wait_if_needed exists

        # Simulate receiving a response
        response_text = f"This is a dummy response to '{prompt[:20]}' from {self.model_name}."
        print(f"  <- Received dummy response: '{response_text[:50]}...'")
        return response_text

    def count_tokens(self, text: str) -> int:
        """Dummy implementation of token counting (e.g., split by space)."""
        tokens = len(text.split())
        print(f"Counting tokens for text '{text[:50]}...': {tokens}")
        return tokens

    # Overriding optional methods
    def get_generation_config(self) -> Dict[str, Any]:
        return {
            "temperature": self._default_temp,
            "default_max_tokens": 100,
        }

    # If you had the wait_if_needed abstract method:
    # def wait_if_needed(self, requested_tokens: int = 0) -> None:
    #     print(f"Dummy check: Pretending to wait if needed for {requested_tokens} tokens.")
    #     # time.sleep(0.1) # Simulate potential delay


# --- How to use the implemented client ---
if __name__ == "__main__":
    try:
        # You cannot instantiate the ABC directly:
        # base_client = BaseLLMClient(model_name="abc-model") # Raises TypeError

        # Instantiate the concrete implementation:
        dummy_client = MyDummyLLMClient(model_name="dummy-pro", temperature=0.7)

        # Use the defined methods:
        prompt_text = "Explain the theory of relativity in simple terms."
        num_tokens = dummy_client.count_tokens(prompt_text)
        response = dummy_client.call_model(prompt_text, max_tokens=150)

        print("\n--- Results ---")
        print(f"Model Used: {dummy_client.model_name}")
        print(f"Prompt Tokens: {num_tokens}")
        print(f"Response: {response}")
        print(f"Default Config: {dummy_client.get_generation_config()}")

    except Exception as e:
        print(f"\nAn error occurred: {e}")
